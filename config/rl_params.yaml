Game:
  participant_name: "NIKOLAS"
  full_path: "/home/ttsitos/catkin_ws/src/hrc_study_tsitosetal/"
  #LfD arguments
  lfd_transfer: True #true if we using learning from Demonstrations transfer learning
  lfd_expert_gameplay: False #true if the expert is playing and collecting demonstration and expert data and place them to the buffers
  lfd_participant_gameplay: True #true if the participant is playing with transfer LfD

  #games_to_capture: #The games that the Demonstrations buffer wil keep from the gameplay with the expert
  train_model: True # True if no training happens
  load_model_training: False # True if loading model to continue training
  load_model_training_dir: "/home/ttsitos/catkin_ws/src/hrc_study_tsitosetal/rl_models/98K_every10_uniform_200ms_NIKOLAS_no_TL_13" # Location of the model 
  load_model_transfer_learning: False # True if loading model for transfer_learning 
  load_model_transfer_learning_dir: "/home/ttsitos/catkin_ws/src/hrc_study_tsitosetal/rl_models/expert" # Location of the model for transfer learning 
  load_model_testing_dir: "/home/ttsitos/catkin_ws/src/hrc_study_tsitosetal/rl_models/98K_every10_uniform_200ms_NIKOLAS_no_TL_13" # Location of the model for testing
  load_demonstrations_data_dir: "/home/ttsitos/catkin_ws/src/hrc_study_tsitosetal/buffers/demo_buffer/demo_data.npy" #Location of the expert experience for the participants buffer will be stored
  #load_demonstrations_data_dir: "/home/nick/catkin_ws/src/hrc_study_tsitosetal/buffers/demo_buffer"
  save: True # Save models and logs
  goal: [-0.264, 0.242] # goal position
  goal_distance: 0.01 # maximum distance from goal in order to win
  goal_velocity: 0.05 # maximum speed in order to win
  win_audio: "street-fighter-ii-you-win-perfect.mp3" # audio for win
  lose_audio: "gaming-sound-effect-hd.mp3" # audio for lose
  start_audio: ["beep-07a.mp3", "beep-09.mp3"] # audio for starting the game
  rest_period: 180 # rest period between training-testing batches
  ppr_threshold: 0.7 # initial ppr probability

SAC:
  # SAC parameters
  layer1_size: 32 # Number of variables in hidden layer
  layer2_size: 32 # Number of variables in hidden layer
  batch_size: 256
  gamma: 0.99  # discount factor
  tau: 0.005
  alpha: 0.0003
  beta: 0.0003
  target_entropy_ratio: 0.4
  buffer_max_size: 1000000


# The game in splitted in training batches. Each batch consists of `test:max_episodes` testing episodes and `learn_every_n_episodes` training episodes.  
# After each batch is completed, the model is trained. This procedure is repeated until `max_episodes` training episodes have been completed. 
Experiment:
  max_episodes: 50  # Total training episodes per game #70 PPR 50 LfD expert #40 LFD participant
  max_duration: 3  #30 Max duration of an episode (in seconds). An episode ends if the ball hits the target or if we reach the time limit
  test_interval: 10 # Test the current model after `test_interval` episodes
  action_duration: 0.2 # Time duration between consecutive RL agent actions
  scheduling: "uniform" # "uniform" or "descending"
  start_training_on_episode: 10 # Will not train the agent before this trial
  stop_random_agent: 10 # Stop using random agent on this trial and start using SAC
  learn_every_n_episodes: 10 # Perform offline gradient updates after every `learn_every_n_episodes` episodes
  total_update_cycles: 9800 #9800 Total number of offline gradient updates throughout the whole experiment
  reward_scale: 2 # Scale for the reward
  number_of_agent_actions: 3 # number of discrete actions
  win_reward: 10 # reward at win
  lose_reward: -1 # reward at every non-win state

  test:
    max_episodes: 10 # Total testing episodes in a training batch
    max_duration: 3 #30 Max duration of an episode test (in seconds). An episode ends if the ball hits the target or if we reach the time limit

